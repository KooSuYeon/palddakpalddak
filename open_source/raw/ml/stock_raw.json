[
    "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)",
    "!sudo apt-get install -y fonts-nanum\n!sudo fc-cache -fv\n!rm ~/.cache/matplotlib -rf\nimport matplotlib as mpl\nimport matplotlib.font_manager as fm\nimport matplotlib.pyplot as plt\n#plt.rc('font', family='NanumSquareRound') ",
    "import matplotlib.pyplot as plt\n\nplt.rc('font', family='NanumSquareRound') ",
    "#목차\n\n### 분석 배경 및 분석 목적\n### 1.데이터셋 준비\n#### 1-1) 결측치, 이상치 확인\n\n###2. EDA\n####2-1) 그래프\n####2-2) 분석 \n\n###3. 시계열 전처리\n#### 3-1) 정상성 확인\n#### 3-2) 정상성을 갖는 데이터로 바꾸기\n#### 3-3) 특성 공학\n\n###4. 모델 생성\n####4-1) 모델 1: OLS\n####4-2) 모델 2: XGBoost Regressor\n#### 4-3) 모델 3: ARIMA\n#### 4-4) 모델 4: XGBoost Classifier\n\n###5. 모델 선택\n####5-1) 하이퍼파라미터 튜닝을 통한 모델 최적화\n####5-2) 최종모델\n\n###6. 특성중요도\n####6-1) 모델 예측에 중요한 특성 설명\n####6-2) 개별 샘플을 예시로 예측 설명\n\n###7. 번외) 다른 ETF 분석 및 포트폴리오 구성\n\n###6. 주요 특성 설명\n\n###7. 묶어서 포트폴리오 만들고 평가",
    "## 분석 배경\n\n저는 데이터 분석회사에서 일하고 있는 직원입니다. 2016년부터 시작된 로보 어드바이저 사업이 이제는 많이 상용화가 되어 여러 증권사/은행/핀테크 기업에서 이미 로보 어드바이저 서비스를 시작하였습니다. 모든 기업이 각자 알고리즘을 개발해서 서비스를 하는 것이 아니라, 다른 데이터 기업과 협업해서 로보 어드바이저 사업을 하는 경우가 있어, 여기에 사용할 알고리즘을 개발하는 프로젝트를 제안하려고 합니다.",
    "## 분석 목표\n\n1. 종가가 20일 지수이동평균를 넘어서 수익을 낼 ETF를 예측한다.\n2. 투자 성향별로 ETF를 분배한 후 이들의 평균수익률을 예측한다.",
    "분석 모델은 시계열 회귀분석과 분류 모델을 설계한 후 더 적절한 모델을 선택할 예정입니다.타겟은 시계열 회귀분석의 경우 ETF의 차분이며, 분류 모델의 경우 타겟은 종가가 20일 지수평균을 초과하는 것입니다.타겟을 20일 지수이동평균 초과하는 것으로 정한 것은, 누적 수익률을 이용한 타겟인 것이 가장 적절하겠으나 분석의 편의성 및 로보 어드바이저의 평균 매매회전율이 높기 때문에(200%이상) 20일을 기준으로 설정하였습니다.\n\n*매매회전율 = (매매대금 * 100) / (운용자금 * 2)",
    "#1. 데이터셋 준비",
    "\n여러 로보 어드바이저의 알고리즘 설명서를 분석한 결과 대부분의 로보 어드바이저는 ETF 위주의 투자를 하는 것을 알 수 있었습니다. 따라서 이번 분석에서는 ETF만을 사용하며, 위험도에 따라서 분류할 예정입니다.\n\n제가 이번 분석에서 사용할 ETF의 선정기준은 시가총액 1000억 원 이상인 것들 중에 분야별(국내 시장지수, 국내 업종테마/국내 파생/해외 주식/원자재/채권)로 고르게 고르며, 생긴지 5년 이상된 지수들입니다.\n\n위험도 분류는 \n* 위험도 1: 채권\n* 위험도 2: 국내 시장지수, 국내 업종테마, 국내 파생\n* 위험도 3: 해외 주식, 원자재\n\n3단계로 하며, 분류기준 역시 다른 알고리즘 설명서를 참고하였습니다. ",
    "ETF 종목 선택은 분야별로 생긴지 10년 이상이 지난 ETF 중 거래량이 제일 많은 하나의 ETF 데이터를 사용할 예정입니다.\n\n* 국내 지수: KODEX200\n* 국내 업종 테마: Kodex 삼성그룹\n* 국내 파생 지수: KODEX 레버리지\n* 해외 지수: TIGER 미국나스닥 100\n* 원자재: KODEX 골드선물(H)\n* 채권: KODEX 단기채권",
    "[벤치마크] Kospi200\n\n앞으로 불러올 ETF들과 추세를 비교하기 위해서 Kospi200 지수를 불러오겠습니다.",
    "import pandas_datareader.data as web\nfrom datetime import datetime\n\nkospi200 = web.DataReader('KPI200', 'naver', start='2011-02-01', end='2021-02-23')\nkospi200",
    "!pip install --upgrade mplfinance",
    "kospi200daily = kospi200[['Open','High','Low','Close','Volume']].iloc[-250:]\nkospi200daily = kospi200daily.astype('float')",
    "import mplfinance as mpf\nmpf.plot(kospi200daily,type='candle',mav=(5,20, 60),\n         style='charles', \n         title='Kospi200, Feb 2020 to Feb 2021',\n         figratio=(29,14),volume=True)",
    "Kospi200 그래프를 보면 지난 1년 간 Kospi200지수의 등락을 볼 수 있습니다. Kospi200 지수는 3월달에 가파르게 하락했다가 계속 완만하게 상승하는 모습을 확인할 수 있으며, 거래량은 증가와 감소를 반복하고 있습니다.",
    "이제 이번 분석에서 사용할 ETF 중 하나인 kodex200을 예시로 어떤 분석방법이 가장 알맞을지 살펴보겠습니다.",
    "## KODEX 200\n\nKODEX200은 국내 Kospi 200 지수를 그대로 추종하는 ETF입니다. 분석의 편의와 여러 ETF 간의 시간 간격을 맞추기 위해 10년 간의 데이터를 불러옵니다.",
    "import pandas_datareader.data as web\nfrom datetime import datetime\n\nkodex200  = web.DataReader('069500', 'naver', start='2011-02-01', end='2021-02-23')\nkodex200",
    "### 결측치 확인",
    "kodex200.info()",
    "제가 사용한 api 특성 상 결측값은 불러와지지 않았습니다.",
    "### 이상치 확인",
    "kodex200 = kodex200.astype('float')\nfig = plt.figure(figsize=(10,5))\nax1 = fig.add_subplot(1, 2, 1)\nax1 = sns.boxplot(kodex200['Close']);\n\nax2 = fig.add_subplot(1, 2, 2)\n\n#ax1.plot(x, y)\n#ax2.bar(x, y)\n\nax2=sns.boxplot(kodex200['Volume']);\n\nplt.show()\n",
    "그래프 상으로는 종가와 거래량에 이상치가 있는 것 같아 보입니다. 이를 아래 캔들차트를 통해서 자세히 알아보겠습니다.",
    "kodex200daily = kodex200[-250:]\nkodex200daily = kodex200daily.astype('float')",
    "mpf.plot(kodex200daily,type='candle',mav=(5,20, 60),\n         style='charles', \n         title='Kodex200, Feb 2020 to Feb 2021',\n         figratio=(15,8),volume=True)\n",
    "KODEX 200 지수의 캔들차트를 보면 KODEX200은 Kospi200를 그대로 추종하기 때문에 Kospi200과 비슷한 추세를 보이는 것을 알 수 있습니다. 지수가 폭락하던 3월에 거래량이 급증한 것만 KOSPI200과 다른 점입니다. \n\n종가는 계속 증가하는 추세이고, 거래량이 40,000,000이상인 값들을 이상치로 처리하기는 어려울 것 같아 그대로 사용하겠습니다.\n\n\n이제 이번 분석에서 사용할 KODEX200의 10년치 추세를 보겠습니다.",
    "sns.set_style('white')\nsns.set_palette('cividis')\nkodex200 = kodex200.astype('float')\nplt.title(\"Kodex200, Feb 2011 to Feb 2021\")\nkodex200['Close'].plot();",
    "10년 간의 KODEX200지수를 보면 전반적으로는 증가하면서도 뚜렷한 계절성 특징을 보인다거나 선형으로 나타내기 어려워 보입니다.\n\n이렇게 시계열 그래프가 긴 주기를 갖는 추세가 있지만 갑작스럽고 예측할 수 없는 방향의 변화가 있을 때는 데이터의 정상성을 확인해야 합니다. 정상성을 따르지 않는 시계열 데이터의 예측은 정확도가 많이 떨어지기 때문에 KODEX200 데이터가 정상성을 띠는지 확인해보겠습니다.\n\n",
    "#시계열 데이터 전처리",
    "###정상성(Stationarity) 확인\n",
    "from statsmodels.tsa.stattools import kpss\nprint(kpss(kodex200['Close'], regression='ct') )",
    "정상성을 확인하는 방법으로 kpss 테스트를 시행했는데, kpss 값은 0.52, p값은 0.01로 KODEX200지수 종가가 정상성을 띤다는 귀무가설을 기각합니다. 즉, KODEX200의 종가는 정상성을 따르지 않기 때문에 시계열분석에 적합하지 않습니다. (정상성 확인 전에 KODEX 200 종가로 모델을 돌렸더니 r2 score가 음수가 나왔습니다...) ",
    "#### 정상성 갖는 데이터로 바꾸기(차분)",
    "차분은 t기간의 데이터에서 t-1기간의 데이터를 뺀 값으로 여기서는 하루 간의 KDOEX200 종가 차이를 구합니다.",
    "#차분 구하기\nkodex200 = kodex200.astype('float')\nkodex200['Diff'] =  np.r_[0, np.diff(kodex200['Close'])]\nkodex200",
    "import seaborn as sns\nimport matplotlib\nmatplotlib.rcParams['axes.unicode_minus'] = False\nsns.set_palette('cividis')\nkodex200['Diff'].plot();",
    "t기간의 KODEX200 종가에서 t-1기간의 KODEX200 종가를 뺀 차분을 구한 후 이를 그래프로 나타낸 것입니다. KODEX200지수 종가와는 다르게 일정한 분산을 보이는 것을 알 수 있습니다.",
    "from statsmodels.tsa.stattools import kpss\nprint(kpss(kodex200['Diff'], regression='ct') )",
    "KPSS 테스트를 해보면 KPSS값 0.084, p값 0.1로, p값이 0.05이상이기 때문에 데이터가 정상성을 따른다는 귀무가설을 기각하지 못합니다. 따라서 차분 데이터를 사용해서 분석을 진행할 수 있습니다.",
    "이어서 차분한 종가의 자기상관을 알아보겠습니다. 많은 시계열 데이터는 고전적 회귀모형의 기본 가정인 오차항들끼리 독립이며, 등분산일 것을 만족하지 못하며 이를 자기상관이라고 부릅니다.\n\n자기상관(autocorrelation)은 시계열의 시차 값(lagged values) 사이의 선형 관계를 측정합니다. 정상성을 만족하지 못하는 KODEX200 종가는 자기상관현상을 보이기 때문에 바로 분석에 이용하기는 어렵습니다. \n\n",
    "import matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(kodex200['Close'])\nplot_pacf(kodex200['Close'])\nplt.show()",
    "KODEX200 종가를 이용해서 그린 acf, pacf 그래프입니다. acf 그래프를 보면 잔차가 일정한 패턴을 보이며 자기상관 현상을 보이고 있습니다.",
    "이와 비교하기 위해 차분한 데이터의 자기상관을 알아보기 위한 acf, pacf 그래프를 그려보겠습니다. ",
    "import matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(kodex200['Diff'])\nplot_pacf(kodex200['Diff'])\nplt.show()",
    "KODEX200종가의 차분 데이터는 잔차가 일정하기 때문에 자기상관현상을 보이지 않습니다. 이는 시계열 데이터가 정상성을 띠면 자기상관함수는 시간에 따라 일정해지기 때문입니다.",
    "그외에도 수익률 데이터로 정상성 테스트를 진행할 수 있습니다. ",
    "#### 정상성을 갖는 데이터로 바꾸기(일일수익률)",
    "kodex200['return'] = kodex200['Close'].pct_change() * 100  #pct_change(5) 5일간 수익률\nkodex200 = kodex200.dropna()",
    "print(kpss(kodex200['return'], regression='ct') )",
    "일일수익률 역시 KPSS 테스트 결과 p값이 0.05이상이기 때문에 귀무가설을 기각하지 못해서 KODEX200의 일일수익률 데이터는 정상성을 보입니다.",
    "import matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(kodex200['return'])\nplot_pacf(kodex200['return'])\nplt.show()",
    "수익률의 acf, pacf 그래프를 살펴보면 차분 데이터와 마찬가지로 자기상관현상을 보이지 않는 것을 알 수 있습니다.",
    "#특성 공학\n\n랜덤 워크 모델이기 때문에 지수 종가를 사용해서 만든 특성을 이용해서 분석합니다. 새롭게 만들 특성은 5일 단순이동평균, 5일 지수이동평균, 이격도, Stochastic %K(Slow), %D(Slow), MACD, RSI입니다. 위의 특성들은 지수 종가와 고가, 저가를 이용한 특성들로 논문을 참고하였습니다. \n\n참고 논문: 하대우(연세대학교) 외. (2019.05).XGBoost 모형을 활용한 코스피 200 주가지수 등락 예측에 관한 연구.한국데이터정보과학회",
    "def engineer(df):\n  df = df.astype('float')\n  \n  #5일 단순이동평균(sma)\n  df['sma5'] = df['Close'].rolling(window=5, min_periods=1).mean()\n\n  \n  #5일 지수이동평균(ema)\n  df['ema5'] = df['Close'].ewm(5).mean()\n  #20일 지수이동평균\n  df['ema20'] = df['Close'].ewm(20).mean()\n\n  #이격도\n  df['disparity'] = df['Close'] - df['sma5']\n  \n  #Stochastic K, D : Fast %K의 m기간 이동평균(SMA)\n  df['fast_k'] = ((df['Close'] - df['Low'].rolling(5).min()) / (df['High'].rolling(5).max() - df['Low'].rolling(5).min())) * 100\n  df['Slow_k'] = df['fast_k'].rolling(3).mean()\n  df['Slow_d'] = df['Slow_k'].rolling(3).mean()\n\n  #MACD\n  df['EMAFast'] = df['Close'].ewm( span = 5, min_periods = 4).mean()\n  df['EMASlow'] = df['Close'].ewm( span = 20, min_periods = 19).mean()\n  df['MACD'] = df['EMAFast'] - df['EMASlow']\n  #df['MACDSignal'] = df['MACD'].ewm( span = 9, min_periods = 8).mean()\n  #df['MACDDiff'] = df['MACD'] - df['MACDSignal']\n\n  #RSI\n  delta = df['Close'].diff(5)\n  delta = delta.dropna()\n\n  up = delta.copy()\n  down = delta.copy()\n\n  up[up<0] = 0\n  down[down>0] = 0\n\n  df['up'] = up\n  df['down'] = down\n\n  AVG_Gain = df['up'].rolling(window = 5).mean()\n  AVG_Loss = abs(df['down'].rolling(window = 5).mean())\n  RS = AVG_Gain/AVG_Loss\n\n  RSI = 100.0 - (100.0/(1+RS))\n  df['RSI'] = RSI \n\n  df['Diff'] =  np.r_[0, np.diff(df['Close'])]\n\n  df = df.drop(columns=['Open', 'Close', 'High','Low', 'fast_k', 'EMAFast', 'EMASlow', 'up', 'down'])\n  df = df.dropna()\n\n  df = df.reset_index() #time_series_split할 때 필요\n\n\n  return df",
    "# 모델 생성\n\n지난 과제를 통해서 주가 예측을 회귀 모델로 할 경우 r2 score가 높더라도 예측 가격이 실제 가격이랑 많이 다르게 나와서 분석이 어렵다고 느꼈습니다. 그렇지만 일단 분류문제로 풀기 전에 회귀분석으로 예측을 해서 수익률 예측을 해보겠습니다.",
    "##회귀분석 모델\n\n* 모델 1: 다중선형회귀모델\n\n* 모델 2: XGBoost 모델\n\n* 모델 3: ARIMA 모델\n\n회귀분석 모델 3가지를 만들 것인데, 타겟은 일일 종가의 차분으로 하고, 평가지표는 MSE, R2 score 두 가지를 사용하겠습니다.",
    "## 테스트 데이터 분리\nCV 이후 사용할 테스트 데이터를 분리합니다. 전체 데이터 중에서 최근 20%를 테스트 데이터로 사용하겠습니다.",
    "len(kodex200)*0.2",
    "kodex200test = kodex200[-495:]\nkodex200train = kodex200.drop(index=kodex200test.index)\nkodex200train.shape, kodex200test.shape",
    "#특성 공학\nkodex200train = engineer(kodex200train)\nkodex200test = engineer(kodex200test)",
    "kodex200train = kodex200train.drop(columns=['return'])\nkodex200test = kodex200test.drop(columns=['return'])",
    "## 모델 1 다중선형회귀분석",
    "타겟을 차분으로 정했기 때문에 선형회귀 모델의 가정인 오차의 정규성, 등분산성, 독립성을 만족합니다.",
    "from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\ntscv = TimeSeriesSplit(n_splits=10)\n\ntarget = 'Diff'\ny =kodex200train[target]\nX =kodex200train.drop(columns = target)\n\nX = X.dropna()\ny= y.dropna()\n\nX = X.drop(columns='Date')\n\npred1 = []\nscores = []\nmses = []\nfor train_index, val_index in tscv.split(kodex200train):\n\n    X_train   = X.iloc[train_index] #drop('Date', axis=1)\n    y_train = y.iloc[train_index]\n\n    X_val    = X.iloc[val_index] \n    y_val  = y.iloc[val_index]\n\n\n    ols = LinearRegression()\n    ols.fit(X_train,y_train)\n\n    preds = ols.predict(X_val)\n    pred1.append(preds)\n    mse = mean_squared_error(y_val, preds)\n    r2score = ols.score(X_val,y_val)\n    mses.append(mse)\n    scores.append(r2score)\n\naverage_r2score = np.mean(scores)\nprint(average_r2score)\nprint(np.mean(mses))",
    "print(\"평균 r2 점수: \", average_r2score)\nprint(\"평균 mse:\",np.mean(mses))",
    "scores",
    "cv = 10일 때 검증 데이터의 평균 r2 score는 0.634이고 mse는 15767입니다. 재밌는 것은 모델에 사용한 데이터가 늘어날수록 평가지표가 안 좋아진다는 점입니다. 2016년 이후 데이터만 사용했을 때 r2 score와 mse 모두 더 좋은 점수였는데, 아무래도 시계열 데이터이다 보니 훈련 데이터가 너무 예전 것인 경우 예측력이 떨어지는 것 같습니다. ",
    "이제 예측값을 그래프에 나타내 보겠습니다.",
    "y_val.plot();",
    "차분 그래프는 예측이 잘 맞는지 알기 어렵기 때문에 다시 종가로 바꿔서 비교해보겠습니다.",
    "new_df1 = kodex200[(kodex200.index >= '2018-05-29')&(kodex200.index <= '2019-02-20')]\nnew_df1",
    "new_df2 = new_df1.drop(columns=['Open','High','Low','Volume','Diff','return'])",
    "val_pred = pd.DataFrame(pred1).T\nval_pred",
    "val_pred.iloc[0,:] = val_pred.iloc[0,:] + 30130.0\nval_pred",
    "val_pred2 = val_pred.cumsum()",
    "val_pred2 = round(val_pred2, 1)\nval_pred2",
    "val_pred2['avg'] = val_pred2.mean(axis=1)\nval_pred2",
    "val_pred2.iloc[:,0].plot()\nval_pred2.iloc[:,1].plot()\nval_pred2.iloc[:,2].plot()\nval_pred2.iloc[:,3].plot()\nval_pred2.iloc[:,4].plot()\nval_pred2.iloc[:,5].plot()\nval_pred2.iloc[:,6].plot()\nval_pred2.iloc[:,7].plot()\nval_pred2.iloc[:,8].plot()\nval_pred2.iloc[:,9].plot()\nval_pred2.iloc[:, 10].plot()\nnew_df2.plot()",
    "예측값과 실제값을 비교해보니 선형모델은 급락을 전혀 예측하지 못하는 것을 알 수 있습니다.\n\n두 그래프를 같이 놓고 보겠습니다.",
    "val_pred2.index = new_df2.index\n",
    "plot_df = pd.concat([new_df2['Close'], val_pred2['avg']], axis=1)\nplot_df",
    "sns.set_palette('PuRd')\nplot_df.plot()",
    "cv로 예측한 값들의 평균과 실제 종가의 그래프입니다. 실제 종가가 2018년 7월부터 급격히 떨어지기 시작하는데 선형모델은 이를 전혀 예측하지 못합니다. 또한, 차분를 예측한 후 이를 누적합해서 종가를 만든 것이기 때문에 차분의 오차가 점점 쌓여서 실제 값과 더욱더 괴리되는 모습을 확인할 수 있습니다. 이런 급락이 있는 경우 모델 성능이 더 떨어지기 때문에 검증 데이터의 값에 따라 모델의 성능이 많이 차이날 것 같습니다.",
    "모델 1의 예측에 중요한 영향을 미친 특성중요도를 살펴보겠습니다.",
    "!pip install eli5",
    "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# permuter 정의\npermuter = PermutationImportance(\n    ols,\n    scoring='r2', # metric\n    n_iter=5, # 다른 random seed를 사용하여 5번 반복\n    random_state=2\n)\n\n\npermuter.fit(X_val, y_val);",
    "feature_names = X_val.columns.tolist()\neli5.show_weights(\n    permuter, \n    top=None, # top n 지정 가능, None 일 경우 모든 특성 \n    feature_names=feature_names # list 형식으로 넣어야 합니다\n)",
    "모델 1에서 차분(Difference)를 예측하는 데 가장 중요한 특성으로는 단순이동평균(5일), 지수이동평균(5일), 지수이동평균(20일), MACD, 이격도가 있습니다. 이동평균이 높은 순위를 차지하고 있는데, 차분이 종가의 하루 차이인 것을 생각해보면, 단순이동평균은 5일 간의 평균이기 때문에 약간의 leakeage가 있다고 볼 수도 있습니다. 그런데 대부분의 랜덤워크 모형은 가격에서 파생된 특성으로 예측을 해서 모형 특성상 발생하는 애매한 부분인 것 같습니다.",
    "## 모델 2 XGBoost Regressor",
    "from xgboost import XGBRegressor\n'''\ntarget = 'Diff'\ny =kodex200train[target]\nX =kodex200train.drop(columns = target)\n\nX = X.dropna()\ny= y.dropna()\n\nX = X.drop(columns='Date')\n'''\nscores_xgb = []\nmse_xgb = []\npreds_xgb = []\n\nfor train_index, val_index in tscv.split(kodex200train):\n\n    X_train2   = X.iloc[train_index] #drop('Date', axis=1)\n    y_train2 = y.iloc[train_index]\n\n    X_val2  = X.iloc[val_index] #.drop('record_date', axis=1)\n    y_val2  = y.iloc[val_index]\n\n    # if needed, do preprocessing here\n\n    xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0,\n                            max_depth=7)\n    xgb_model.fit(X_train2, y_train2)\n\n    pred_xgb = xgb_model.predict(X_val2)\n    preds_xgb.append(pred_xgb)\n\n    r2score_xgb = xgb_model.score(X_val2,y_val2)\n    scores_xgb.append(r2score_xgb)\n\n    mse_xg = mean_squared_error(y_val2, pred_xgb)\n    mse_xgb.append(mse_xg)\naverage_r2score_xgb = np.mean(scores_xgb)\nprint(\"평균 r2 score: \",average_r2score_xgb)\nprint(\"평균 mse: \",np.mean(mse_xgb))",
    "모델 2의 r2 score는 0.574이고 mse는 17490으로 모델 1보다 r2 score는 낮고 mse는 더 큽니다. 따라서 모델 1이 오류가 더 적은 모델이라고 할 수 있습니다. \n",
    "import eli5\nfrom eli5.sklearn import PermutationImportance\n\n# permuter 정의\npermuter2 = PermutationImportance(\n    xgb_model,\n    scoring='r2', # metric\n    n_iter=5, \n    random_state=2\n)\n\n\npermuter2.fit(X_val2, y_val2);",
    "feature_names2 = X_val2.columns.tolist()\neli5.show_weights(\n    permuter2, \n    top=None, \n    feature_names=feature_names2 \n)",
    "모델 2에서 차분(Difference)을 예측하는 데 가장 중요한 특성으로는 이격도, Stochastic %k, MACD, Stochastic %D가 있습니다. 이격도가 가장 중요한 특성으로 나왔는데, 이격도를 구하는데 종가와 이동평균이 필요한 것을 생각해보면 순위가 낮게 나온 sma5는 특성에서 제외해도 될 것 같습니다. ",
    "##모델 3 ARIMA",
    "시계열 예측에 많이 사용하는 ARIMA 모델을 사용해서 분석을 해보겠습니다. ARIMA 모델은 AR모델과 MA모델을 합친 것으로 p,d,q 세가지의 모수를 가집니다. p는 AR모형의 lag, d는 차분 횟수, q는 MA모형의 lag를 의미합니다. 정상성 확인하는 과정에서의 ACF, PACF 그래프를 보고 p = 1, d=1, q=0으로 유추하였습니다.",
    "#모델1,2에서 사용된 훈련 데이터 + 검증 데이터\narima_df = kodex200[(kodex200.index<='2019-02-21') & (kodex200.index>'2011-03-15')] ",
    "from statsmodels.tsa.arima_model import ARIMA\nmod = ARIMA(arima_df['Diff'], order=(1, 1, 0)) \nres = mod.fit(trend='c',full_output=True, disp=0)\nprint(res.summary())",
    "residuals = pd.DataFrame(res.resid)\nfig = plt.figure(figsize=(10,5))\n\n#ax1 = fig.add_subplot(1, 2, 1)\nax1 = residuals.plot(title=\"Residuals\")\n\n#ax2 = fig.add_subplot(1, 2, 2)\nax2 = residuals.plot(kind='kde', title='Density')\n\nplt.show()",
    "잔차그림을 그려봤을 때 어떤 경향도 보이지 않고 있습니다.",
    "plt.style.use('ggplot')\nres.plot_predict(dynamic=False)\nplt.show()",
    "ARIMA 모델로 예측한 값과 실제 차분의 그래프입니다. 이대로는 분석이 어렵고, 투명도 조절을 하거나 종가로 바꿔서 봐야할 것 같습니다.",
    "이렇게 회귀분석 모델 세가지를 이용한 분석을 했는데, 회귀분석로는 의사결정 시그널을 주기 어려운 것 같습니다.\n물론 차분을 예측해서 누적합을 하면 지수의 종가 예측값이 나오는데, 오차범위도 정해야 하고 모델이 너무 복잡해집니다.\n\n이번엔 의사결정에 직접적으로 도움을 줄 수 있는 분류 모델로 다시 분석해 볼 것입니다. ",
    "## 분류모델\n",
    "\n## 모델 4 XGBoost Classifier\n\n타겟은 종가가 ema20선보다 큰 값은 1, 아닌 값은 0으로 하겠습니다.",
    "def engineer2(df):\n  df = df.astype('float')\n  \n  #5일 단순이동평균(sma)\n  df['sma5'] = df['Close'].rolling(window=5, min_periods=1).mean()\n\n  \n  #5일 지수이동평균(ema)\n  df['ema5'] = df['Close'].ewm(5).mean()\n  #20일 지수이동평균\n  df['ema20'] = df['Close'].ewm(20).mean()\n\n  #이격도\n  df['disparity'] = df['Close'] - df['sma5']\n  \n  #Stochastic K, D : Fast %K의 m기간 이동평균(SMA)\n  df['fast_k'] = ((df['Close'] - df['Low'].rolling(5).min()) / (df['High'].rolling(5).max() - df['Low'].rolling(5).min())) * 100\n  df['Slow_k'] = df['fast_k'].rolling(3).mean()\n  df['Slow_d'] = df['Slow_k'].rolling(3).mean()\n\n  #MACD\n  df['EMAFast'] = df['Close'].ewm( span = 5, min_periods = 4).mean()\n  df['EMASlow'] = df['Close'].ewm( span = 20, min_periods = 19).mean()\n  df['MACD'] = df['EMAFast'] - df['EMASlow']\n  #df['MACDSignal'] = df['MACD'].ewm( span = 9, min_periods = 8).mean()\n  #df['MACDDiff'] = df['MACD'] - df['MACDSignal']\n\n  #RSI\n  delta = df['Close'].diff(5)\n  delta = delta.dropna()\n\n  up = delta.copy()\n  down = delta.copy()\n\n  up[up<0] = 0\n  down[down>0] = 0\n\n  df['up'] = up\n  df['down'] = down\n\n  AVG_Gain = df['up'].rolling(window = 5).mean()\n  AVG_Loss = abs(df['down'].rolling(window = 5).mean())\n  RS = AVG_Gain/AVG_Loss\n\n  RSI = 100.0 - (100.0/(1+RS))\n  df['RSI'] = RSI \n\n  df['Diff'] =  np.r_[0, np.diff(df['Close'])]\n\n  df = df.drop(columns=['Open', 'High','Low', 'fast_k', 'EMAFast', 'EMASlow', 'up', 'down'])\n  df = df.dropna()\n\n  df = df.reset_index() #time_series_split할 때 필요\n\n\n  return df",
    "#타겟값 만들기\ndef define_target_condition(df):\n \n    # price above trend multiple days later\n    df['target_cls'] = np.where(df['Close'].shift(-20) > df.ema20.shift(-20), 1, 0)\n\n    # important, remove NaN values\n    df=df.fillna(0).copy()\n    \n    df.tail()\n    \n    return df",
    "kodex200test2 = kodex200[-495:]\nkodex200train2 = kodex200.drop(index=kodex200test2.index)\nkodex200train2.shape, kodex200test2.shape",
    "kodex200train2 = engineer2(kodex200train2)\nkodex200test2 = engineer2(kodex200test2)\nkodex200train2 =define_target_condition(kodex200train2)\nkodex200test2 = define_target_condition(kodex200test2)",
    "kodex200train2",
    "베이스라인",
    "kodex200train2['target_cls'].value_counts(normalize=True)",
    "print('베이스라인 모델 검증 정확도: ', 0.543556)",
    "타겟의 분포는 고른 편이기 때문에 모델 생성시 타겟 클래스 비율을 고려할 필요는 없을 것 같습니다.",
    "eval_set = [(X_train2, y_train2), \n            (X_val2, y_val2)]\n\nmodel.fit(X_train2, y_train2, \n          eval_set=eval_set,\n          eval_metric='error', # #(wrong cases)/#(all cases)\n          early_stopping_rounds=50\n         ) # 50 rounds 동안 스코어의 개선이 없으면 멈춤",
    "kodex200train2 = kodex200train2.drop(columns = ['return','Diff'])",
    "from xgboost import XGBClassifier\n#from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n\ntarget2 = 'target_cls'\n\ny2 =kodex200train2[target2]\nX2 =kodex200train2.drop(columns = target2)\n\nX2 = X2.dropna()\ny2= y2.dropna()\n\nX2 = X2.drop(columns='Date')\n\nscores_xgbc = []\npreds_xgbc = []\nf1_score1 = []\nprec_score = []\nrec_score = []\n\nfor train_index, val_index in tscv.split(kodex200train2):\n\n    X2_train   = X2.iloc[train_index] #drop('Date', axis=1)\n    y2_train = y2.iloc[train_index]\n\n    X2_val  = X2.iloc[val_index] #.drop('record_date', axis=1)\n    y2_val  = y2.iloc[val_index]\n\n    # if needed, do preprocessing here\n\n    xgb = XGBClassifier(n_estimators=1000, learning_rate=0.08, gamma=0,\n                            max_depth=7)\n    #xgb.fit(X2_train, y2_train, eval_metric='error', early_stopping_rounds=50)\n    eval_set = [(X2_train, y2_train), \n            (X2_val, y2_val)]\n\n    xgb.fit(X2_train, y2_train, \n          eval_set=eval_set,\n          eval_metric='error', # #(wrong cases)/#(all cases)\n          early_stopping_rounds=50\n         ) \n\n    pred_xgbc = xgb.predict(X2_val)\n    preds_xgbc.append(pred_xgbc)\n\n    score_xgbc = xgb.score(X2_val,y2_val)\n    scores_xgbc.append(score_xgbc)\n\n   # preci = precision_score(X2_val,pred_xgbc)\n    #prec_score.append(preci)\n\n    #recall_s = recall_score(X2_val, pred_xgbc)\n    #rec_score.append(recall_s)\n\n    #f1 = f1_score(X2_val, pred_xgbc)\n    #f1_score1.append(f1)\n\naverage_score_xgbc = np.mean(scores_xgbc)\n#average_f1_score = np.mean(f1_score1)\nprint(\"평균 accuracy score: \",average_score_xgbc)\n#print(\"평균 f1: \",np.mean(average_f1_score))",
    "모델 4의 검증 정확도는 0.53으로 베이스라인 모델의 0.54보다 낮은 편입니다. 이를 GridSearchCv로 개선할 수 있는지 확인해 보겠습니다.",
    "# 모델 최적화\n\nGridSearchCv를 이용하여 모델 4의 하이퍼 파라미터 튜닝을 진행하겠습니다.",
    "from xgboost import XGBClassifier\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nimport numpy as np\n\nxgb_f = xgb.XGBClassifier()\nparam_search = {'booster' :['gbtree'],\n                'max_depth' : [3, 5, 10],\n                'gamma':[0,1,2,3],\n                'n_estimators':[50, 100, 1000],\n                'objective':['binary:logistic'],\n                'random_state':[2]}\n\n#tscv = TimeSeriesSplit(n_splits=10)\ngsearch = GridSearchCV(estimator=xgb_f, cv=tscv,\n                        param_grid=param_search, scoring ='f1', verbose=1, n_jobs=-1)\n\ngsearch.fit(X2_train, y2_train)\n            ",
    "print('최적 하이퍼파라미터: ', gsearch.best_params_)\nprint('f1: ', gsearch.best_score_)",
    "위에서 나온 최적 하이퍼 파라미터 근처의 값으로 다시 gridsearchcv를 시행해보겠습니다.",
    "xgb_f = xgb.XGBClassifier()\nparam_search = {'booster' :['gbtree'],\n                'max_depth' : [5, 7],\n                'gamma':[3, 4,5],\n                'n_estimators':[100, 150, 200],\n                'objective':['binary:logistic'],\n                'random_state':[2]}\n\n\ngsearch = GridSearchCV(estimator=xgb_f, cv=tscv,      #모델 1부터 쓰던 timeseries cv를 사용\n                        param_grid=param_search, scoring ='f1', verbose=1, n_jobs=-1)\n\ngsearch.fit(X2_train, y2_train)\n\nprint('최적 하이퍼파라미터: ', gsearch.best_params_)\nprint('f1: ', gsearch.best_score_)",
    "처음에 시행한 gridsearchcv와 같은 하이퍼파라미터들이 선택되었습니다. 이를 사용하여 최종 모델을 만들겠습니다.",
    "## 최종 모델",
    "모델 4를 최적화한 모델을 최종 모델로 선정하여 검증세트 정확도와 f1 score를 확인하겠습니다.",
    "pipe = gsearch.best_estimator_",
    "y_pred_v = pipe.predict(X2_val)\naccu_grid = accuracy_score(y2_val, y_pred_v)\nf1_grid = f1_score(y2_val, y_pred_v)\nprint(f'검증세트 정확도: {accu_grid:,.3f}')\nprint(f'검증세트 f1 score: {f1_grid:,.3f}')",
    "최종 모델은 점수가 많이 낮은 편이지만, 비교적 최근인 검증세트까지 넣어서 훈련하면 성능이 더 좋아질 수도 있기 때문에,일단 검증세트까지 다시 넣어서 gridsearch를 다시 시행하겠습니다.",
    "X2_train.columns",
    "X_train3 = kodex200train2.drop(columns=['Date','target_cls'])\ny_train3 = kodex200train2['target_cls']\nX_train3.shape, y_train3.shape",
    "from xgboost import XGBClassifier\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nimport numpy as np\n\nxgb_f = xgb.XGBClassifier()\nparam_search = {'booster' :['gbtree'],\n                'max_depth' : [3, 5, 10],\n                'gamma':[0,1,2,3],\n                'n_estimators':[50, 100, 1000],\n                'objective':['binary:logistic'],\n                'random_state':[2]}\n\n#tscv = TimeSeriesSplit(n_splits=10)\ngsearch = GridSearchCV(estimator=xgb_f, cv=tscv,\n                        param_grid=param_search, scoring ='f1', verbose=1, n_jobs=-1)\n\ngsearch.fit(X_train3, y_train3)",
    "kodex200test2\n\nX_test = kodex200test2.drop(columns=['Date','target_cls'])\ny_test = kodex200test2['target_cls']\nX_test.shape, y_test.shape",
    "X_test = X_test.drop(columns=['Diff','return'])",
    "pipe2 = gsearch.best_estimator_",
    "y_pred = pipe2.predict(X_test)\naccu_grid2 = accuracy_score(y_test, y_pred)\nf1_grid2 = f1_score(y_test, y_pred)\nprint(f'테스트세트 정확도: {accu_grid2:,.3f}')\nprint(f'테스트세트 f1 score: {f1_grid2:,.3f}')",
    "검증세트까지 넣어서 만든 모델의 테스트세트 정확도와 f1 score 모두 오른 것을 확인할 수 있습니다. 시계열 분석을 할 때는 무조건 데이터가 많다고 좋은 것이 아니 때문에 훈련데이터, 검증데이터, 테스트데이터를 나누는 기준이 중요할 것 같습니다. 이번 분석에서는 총 10년 간의 데이터 중에서 테스트데이터를 최근 20%로 잡고 모델링했는데, 데이터 크기를 더 줄이면(최신 데이터로 학습) 성능이 더 좋아지지 않을까 생각됩니다.",
    "ROC curve와 AUC 점수 확인",
    "#ROC curve\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n\nplt.scatter(fpr, tpr, color='#413c69')\nplt.plot(fpr, tpr, color='#bd2000')\nplt.title('ROC curve')\nplt.xlabel('FPR')\nplt.ylabel('TPR')",
    "from sklearn.metrics import roc_auc_score\n\ny_pred_proba = pipe2.predict_proba(X_test)[:, -1]\nprint('AUC score: ', roc_auc_score(y_test, y_pred_proba))",
    "ROC Curve를 보면 대각선보다는 그래프가 오목한 것(우측 하단 기준)을 알 수 있습니다. 하지만 AUC 점수가 0.63인 것을 보아 성능이 아주 좋아보이지는 않습니다.",
    "# 특성중요도",
    "# permuter 정의\npermuter_f = PermutationImportance(\n    pipe2, # model\n    scoring='f1', # metric\n    n_iter=5, # 다른 random seed를 사용하여 5번 반복\n    random_state=2\n)\n\npermuter_f.fit(X_test, y_test);",
    "feature_names_f = X_test.columns.tolist()\npd.Series(permuter_f.feature_importances_, feature_names_f).sort_values()\n\n# 특성별 score 확인  #여러 번 섞은 결과임\neli5.show_weights(\n    permuter_f, \n    # top n 지정 가능, None 일 경우 모든 특성 \n    feature_names=feature_names_f # list 형식으로 넣어야 합니다\n)",
    "모델 4의 경우 타겟을 예측하는 데 가장 중요한 특성으로는 20일 지수이동평균, MACD, 종가, 이격도, Stochastic %k 등이 있습니다. 이때 20일 지수이동평균은 타겟을 만드는 데 사용하였기 때문에 Leakage 문제가 있습니다. 하지만, 타겟을 미래의 종가가 20일 지수이동평균 보다 높은 지를 예측하는 것으로 했기 때문에 모델 설계상 필연적으로 발생할 수밖에 없었던 문제 같습니다. 거래량이 낮은 상관관계를 보이는 것이 특이했고, 모델 1,2,3과 다르게 5일 단순이동평균(sma5)이 가장 덜 중요한 특성으로 나온 것이 특이점입니다.",
    "이제 개별 샘플을 예시로 특성 설명을 하겠습니다.",
    "X_test.tail(5)",
    "!pip install shap",
    "row = X_test.iloc[[-1]]\npipe2.predict(row)",
    "마지막 샘플(2021년 2월 23일)의 타겟 예측값은 0(종가가 20일 지수이동평균 보다 낮다)입니다. 이러한 예측에 영향을 준 특성들을 살펴보겠습니다. ",
    "import shap\n\nexplainer = shap.TreeExplainer(pipe2)\nshap_values = explainer.shap_values(row)\n\nshap.initjs()\nshap.force_plot(\n    base_value=explainer.expected_value, \n    shap_values=shap_values,       \n    features=row\n)",
    "예측하는 데 긍정적 영향을 준 것은 5일 지수이동평균, 종가이고 부정적 영향을 준 것은 5일 단순이동평균과 20일 지수이동평균입니다. ",
    "이번 분석을 하면서 느꼈던 아쉬운 점과 개선점을 생각해봤습니다.\n\n아쉬운 점 및 개선방법\n\n1. 시계열 데이터 분석이 미숙하여 회귀 분석 모델을 활용하지 못한 것\n모델 1, 2, 3은 회귀 분석 모델인데 시계열 데이터의 cv 방법이 익숙하지 않고, 데이터를 정상화해서 타겟 설정이 어려운 것과 ARIMA 모델은 처음 사용해 보는 등 과제를 수행하는 데 있어 여러 어려움이 있었습니다. 다음에 시계열 분석을 시도한다면 타겟을 오차범위 내 수익률 등으로 바꾸면 해석이 더 용이할 것 같습니다.\n\n2. 시계열 분석에 머신러닝이 적합한지에 대한 의문. \n많은 블로그를 보면 시계열 예측에 트리기반 모델 등 머신러닝 모델보다는 LSTM 같은 딥러닝 모델을 많이 사용하는 것 같습니다. 지난 과제에서 집값 예측 등 시계열 분석에 사용한 회귀분석 등이 있었지만, 랜덤워크 모형처럼 가격을 기반으로 예측할 때는 다른 모델이 더 적절할 것 같습니다. ",
    "머신러닝을 이용한 분석은 여기까지입니다. 아래의 분석은 제가 설정한 시나리오 상 20일 간 지수이동평균을 넘을 것이라고 분류된 ETF들을 어떻게 포트폴리오로 구성할 지에 대한 것입니다.",
    "#다른 ETF 분석",
    "국내 지수 ",
    "tiger200  = web.DataReader('102110', 'naver', start='2011-02-01', end='2021-02-23')\ntiger200\n",
    "국내 업종/테마 ",
    "kodex_samsung = web.DataReader('102780', 'naver', start='2016-12-27', end='2021-02-23')\nkodex_samsung",
    "국내 파생",
    "kodex200_lev2 = web.DataReader('122630', 'naver', start='2011-02-01', end='2021-02-23')\nkodex200_lev2",
    "해외 주식",
    "tiger_nasdaq100 = web.DataReader('133690', 'naver', start='2011-02-01', end='2021-02-23')\ntiger_nasdaq100",
    "원자재",
    "kodex_gold = web.DataReader('132030', 'naver', start='2011-02-01', end='2021-02-23')\nkodex_gold",
    "채권",
    "kodex_bond_short = web.DataReader('153130', 'naver', start='2011-02-01', end='2021-02-23')\nkodex_bond_short",
    "#포트폴리오 구성",
    "assets = [ 'KODEX200', 'KODEX_Samsung', 'KODEX_WTI', 'KODEX_Bond_Short', 'TIGER_Nasdaq', 'KODEX_LEV']",
    "weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2]) #여기 일단 다섯개",
    "#각 etf별 종가 df로 만들기\ndf = pd.concat([kodex_gold['Close'], tiger_nasdaq100['Close'], kodex200_lev2['Close'], kodex_samsung['Close'], tiger200['Close']],\n               keys=['Kodex_gold','Tiger_nasdaq','Kodex200_lev2','Kodex_samsung','Tiger200'],axis =1)\ndf",
    "df = df.dropna()\ndf",
    "df = df.astype('float')",
    "#시각화\ntitle = 'Portfolio'\n\nmy_etf = df\nplt.figure(figsize=(12,5))\n\nfor c in my_etf.columns.values:\n  plt.plot(my_etf[c], label=c)\n\nplt.title(title)\nplt.xlabel('Date', fontsize=15)\nplt.ylabel('Close price')\nplt.legend(my_etf.columns.values, loc = 'upper left')\nplt.show()",
    "#일일수익률\nreturns = df.pct_change()\nreturns",
    "returns = returns.dropna()",
    "#연간 공분산 행렬\ncov_matrix_annual = returns.cov() * 251\ncov_matrix_annual",
    "#포트폴리오 분산의 기댓값 = weight.T dot (공분산행렬) dot weight\nport_variance = np.dot(weights.T, np.dot(cov_matrix_annual, weights))\nport_variance",
    "#포트폴리오 변동성의 기댓값 = 분산의 기댓값의 제곱근(표준편차)\nport_volatility = np.sqrt(port_variance)\nport_volatility",
    "#포트폴리오 연간 단순 수익률\nport_annual_return = np.sum(returns.mean()*weights)*251",
    "#예상 연간 수익, 변동성 또는 위험 및 분산\npercent_var = str(round(port_variance, 2) * 100)\npercent_vols = str(round(port_volatility, 2) * 100)\npercent_ret = str(round(port_annual_return, 2)* 100)\nprint(\"예상 연간수익률: \", percent_ret, '%')\nprint(\"예상 변동성: \", percent_vols, '%')\nprint(\"예상 분산: \", percent_var, '%')",
    "!pip install PyPortfolioOpt",
    "#포트폴리오 최적화\nfrom pypfopt.efficient_frontier import EfficientFrontier \nfrom pypfopt import risk_models\nfrom pypfopt import expected_returns",
    "mu = expected_returns.mean_historical_return(df)\ns = risk_models.sample_cov(df)",
    "ef = EfficientFrontier(mu,s)\nweights = ef.max_sharpe()\ncleaned_weights = ef.clean_weights()\nprint(cleaned_weights)\nef.portfolio_performance(verbose=True)",
    "펀드의 위험성을 평가하기 위해 샤프 비율을 사용합니다. 샤프 비율은 투자자가 부담하는 위험을 자산 수익률이 얼마나 잘 보상하는지를 규정합니다. 두 자산을 공동의 기준지표와 비교할 경우, 더 높은 샤프 비율을 나타내는 자산이 동일한 위험에 대해 더 높은 수익률을 제공합니다.\n\n샤프 비율은 기간에 따라 달라질 수 있습니다. 일일수익률, 주간수익률, 1개월수익률 모두 샤프비율을 구할 수 있습니다.",
    "#개별 종목 할당\n!pip install pulp",
    "from pypfopt.discrete_allocation import DiscreteAllocation, get_latest_prices\n\nlatest_prices = get_latest_prices(df)\nweights = cleaned_weights\n\nda = DiscreteAllocation(weights, latest_prices, total_portfolio_value=1000000)\nallocation, leftover = da.lp_portfolio()\n\nprint(\"자산 분배: \", allocation)\nprint(\"남은 계좌 잔액: \\{:.2f}\".format(leftover))",
    "다섯 가지 분야에 골고루 투자하는 위험중립형 투자자의 경우 최적화된 포트폴리오의 추천은 kodex골드, tiger나스닥100, kodex 삼성그룹을 각각 26, 8, 14개 매수할 것을 추천하고 있습니다.\n"
]