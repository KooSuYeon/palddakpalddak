[
    "ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\nollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "pulling manifest\n...\nverifying sha256 digest\nwriting manifest\nsuccess\n",
    "pip install ollama\n",
    "dataset = []\nwith open('cat-facts.txt', 'r') as file:\n  dataset = file.readlines()\n  print(f'Loaded {len(dataset)} entries')\n",
    "import ollama\n\nEMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\nLANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n\n# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\nVECTOR_DB = []\n\ndef add_chunk_to_database(chunk):\n  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n  VECTOR_DB.append((chunk, embedding))\n",
    "for i, chunk in enumerate(dataset):\n  add_chunk_to_database(chunk)\n  print(f'Added chunk {i+1}/{len(dataset)} to the database')\n",
    "def cosine_similarity(a, b):\n  dot_product = sum([x * y for x, y in zip(a, b)])\n  norm_a = sum([x ** 2 for x in a]) ** 0.5\n  norm_b = sum([x ** 2 for x in b]) ** 0.5\n  return dot_product / (norm_a * norm_b)\n",
    "def retrieve(query, top_n=3):\n  query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n  # temporary list to store (chunk, similarity) pairs\n  similarities = []\n  for chunk, embedding in VECTOR_DB:\n    similarity = cosine_similarity(query_embedding, embedding)\n    similarities.append((chunk, similarity))\n  # sort by similarity in descending order, because higher similarity means more relevant chunks\n  similarities.sort(key=lambda x: x[1], reverse=True)\n  # finally, return the top N most relevant chunks\n  return similarities[:top_n]\n",
    "input_query = input('Ask me a question: ')\nretrieved_knowledge = retrieve(input_query)\n\nprint('Retrieved knowledge:')\nfor chunk, similarity in retrieved_knowledge:\n  print(f' - (similarity: {similarity:.2f}) {chunk}')\n\ninstruction_prompt = f'''You are a helpful chatbot.\nUse only the following pieces of context to answer the question. Don't make up any new information:\n{'\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}\n'''\n",
    "stream = ollama.chat(\n  model=LANGUAGE_MODEL,\n  messages=[\n    {'role': 'system', 'content': instruction_prompt},\n    {'role': 'user', 'content': input_query},\n  ],\n  stream=True,\n)\n\n# print the response from the chatbot in real-time\nprint('Chatbot response:')\nfor chunk in stream:\n  print(chunk['message']['content'], end='', flush=True)\n",
    "python demo.py\n",
    "Ask me a question: tell me about cat speed\nRetrieved chunks: ...\nChatbot response:\nAccording to the given context, cats can travel at approximately 31 mph (49 km) over a short distance. This is their top speed.\n"
]