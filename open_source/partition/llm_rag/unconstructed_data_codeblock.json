[
    "!pip install -q torch transformers accelerate bitsandbytes sentence-transformers unstructured[all-docs] langchain chromadb langchain_community",
    "!mkdir -p \"./documents\"\n!wget https://www.gov.nl.ca/ecc/files/env-protection-pesticides-business-manuals-applic-chapter7.pdf -O \"./documents/env-protection-pesticides-business-manuals-applic-chapter7.pdf\"\n!wget https://ipm.ifas.ufl.edu/pdfs/Citrus_IPM_090913.pptx -O \"./documents/Citrus_IPM_090913.pptx\"\n!wget https://www.gutenberg.org/ebooks/45957.epub3.images -O \"./documents/45957.epub\"\n!wget https://blog.fifthroom.com/what-to-do-about-harmful-garden-and-plant-insects-and-pests.html -O \"./documents/what-to-do-about-harmful-garden-and-plant-insects-and-pests.html\"",
    "# Optional cell to reduce the amount of logs\n\nimport logging\n\nlogger = logging.getLogger(\"unstructured.ingest\")\nlogger.root.removeHandler(logger.root.handlers[0])",
    ">>> import os\n\n>>> from unstructured.ingest.connector.local import SimpleLocalConfig\n>>> from unstructured.ingest.interfaces import PartitionConfig, ProcessorConfig, ReadConfig\n>>> from unstructured.ingest.runner import LocalRunner\n\n>>> output_path = \"./local-ingest-output\"\n\n>>> runner = LocalRunner(\n...     processor_config=ProcessorConfig(\n...         # logs verbosity\n...         verbose=True,\n...         # the local directory to store outputs\n...         output_dir=output_path,\n...         num_processes=2,\n...     ),\n...     read_config=ReadConfig(),\n...     partition_config=PartitionConfig(\n...         partition_by_api=True,\n...         api_key=\"YOUR_UNSTRUCTURED_API_KEY\",\n...     ),\n...     connector_config=SimpleLocalConfig(\n...         input_path=\"./documents\",\n...         # whether to get the documents recursively from given directory\n...         recursive=False,\n...     ),\n... )\n>>> runner.run()",
    "INFO: NumExpr defaulting to 2 threads.\n",
    "!brew install poppler\n!brew install tesseract",
    "from unstructured.staging.base import elements_from_json\n\nelements = []\n\nfor filename in os.listdir(output_path):\n    filepath = os.path.join(output_path, filename)\n    elements.extend(elements_from_json(filepath))",
    "from unstructured.chunking.title import chunk_by_title\n\nchunked_elements = chunk_by_title(\n    elements,\n    # maximum for chunk size\n    max_characters=512,\n    # You can choose to combine consecutive elements that are too small\n    # e.g. individual list items\n    combine_text_under_n_chars=200,\n)",
    "from langchain_core.documents import Document\n\ndocuments = []\nfor chunked_element in chunked_elements:\n    metadata = chunked_element.metadata.to_dict()\n    metadata[\"source\"] = metadata[\"filename\"]\n    del metadata[\"languages\"]\n    documents.append(Document(page_content=chunked_element.text, metadata=metadata))",
    "from langchain_community.vectorstores import Chroma\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\nfrom langchain.vectorstores import utils as chromautils\n\n# ChromaDB doesn't support complex metadata, e.g. lists, so we drop it here.\n# If you're using a different vector store, you may not need to do this\ndocs = chromautils.filter_complex_metadata(documents)\n\nembeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\nvectorstore = Chroma.from_documents(documents, embeddings)\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})",
    "from huggingface_hub import notebook_login\n\nnotebook_login()",
    "from langchain.prompts import PromptTemplate\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import pipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom langchain.chains import RetrievalQA",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nterminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n\ntext_generation_pipeline = pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    temperature=0.2,\n    do_sample=True,\n    repetition_penalty=1.1,\n    return_full_text=False,\n    max_new_tokens=200,\n    eos_token_id=terminators,\n)\n\nllm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\nprompt_template = \"\"\"\n<|start_header_id|>user<|end_header_id|>\nYou are an assistant for answering questions using provided context.\nYou are given the extracted parts of a long document and a question. Provide a conversational answer.\nIf you don't know the answer, just say \"I do not know.\" Don't make up an answer.\nQuestion: {question}\nContext: {context}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n\n\nqa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type_kwargs={\"prompt\": prompt})",
    "question = \"Are aphids a pest?\"\n\nqa_chain.invoke(question)[\"result\"]",
    "Yes, aphids are considered pests because they feed on the nutrient-rich liquids within plants, causing damage and potentially spreading disease. In fact, they're known to multiply quickly, which is why it's essential to control them promptly. As mentioned in the text, aphids can also attract ants, which are attracted to the sweet, sticky substance they produce called honeydew. So, yes, aphids are indeed a pest that requires attention to prevent further harm to your plants!"
]