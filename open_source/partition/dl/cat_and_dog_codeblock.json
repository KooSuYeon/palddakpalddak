[
    "import os\nimport random\nimport time\nfrom PIL import Image\n\nimport cv2\nimport numpy as np\nimport timm\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms\n\nimport torchsummaryX\n",
    "# -- coding: utf8 --\n'''ResNet을 이용한 개 고양이 분류기 '''\n''' ####################### 1. 라이브러리 ####################### '''\n# load to CPU or GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available() == True:\n    print(\"\\n### cuda check ###\")\n    print(device)                                                         \n    print('Count of using GPUs:', torch.cuda.device_count())    \n    print('Current cuda device:', torch.cuda.current_device())                          \n\n# from PIL import Image                               # 기본 이미지 처리 라이브러리. 다양한 이미지 파일 형식 접근, 이미지 조작 제공\nfrom torch.utils.data import DataLoader, Dataset    # 훈련모델 데이터셋 관리와 조정에 쓰이는 라이브러리\n# Dataset: 샘플과 정답(label)을 저장, DataLoader: Dataset 을 샘플에 쉽게 접근할 수 있도록 순회 가능한 객체(iterable)로 감싼다\n\n",
    "\n''' ####################### 2. 이미지 전처리 ####################### '''\n\n\ndef normalize_image(img):\n    img = transforms.ToTensor()(img)\n    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n    std = torch.Tensor([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n    img = (img - mean) / std\n    return img\n\n\n\ndef custom_transform(resize, mean, std, is_train):     # 내가 만든 함수(transform.Compose 안 쓴다)\n    transform_list = []\n    \n    if is_train:    # 훈련용\n        transform_list.append(transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)))\n        transform_list.append(transforms.RandomHorizontalFlip())\n    else:           # 검증용\n        transform_list.append(transforms.Resize(256))\n        transform_list.append(transforms.CenterCrop(resize))\n        \n    transform_list.append(transforms.ToTensor())\n    transform_list.append(transforms.Normalize(mean, std))\n    \n    def apply_transform(x):\n        for transform in transform_list:\n            x = transform(x)\n        return x\n    \n    return apply_transform\n\n\nclass ImageTransform():                     # 책에 있는 transforms.Compose 쓰는 함수\n    def __init__(self, resize, mean, std):\n        # transform에서 사용하는 파라미터는 다음 5개가 있다.\n        self.data_transform = {     # 딕셔너리로 구분할 수 있게 만들었다. \n            'train': custom_transform(resize, mean, std, is_train= True),   # 훈련용\n            'val': custom_transform(resize, mean, std, is_train = False)    # 검증용\n            }\n        \n    def __call__(self, img, phase):                 \n        # __call__: 클래스를 호출할 수 있도록 하는 메서드. \n        # __init__이 인스턴트 초기화에 쓰이는 생성자라면 __call__은 호출하면 구냥 __call__의 return값을 반환한다.\n        return self.data_transform[phase](img)\n\n\n\n    # 이미지 사이즈가 다 다르다. 같은 이미지 사이즈로 전처리가 필요함.\n\n\n# 변수에 대한 값 정의\nsize = 224                      # 224 x 224 이미지로 만들꺼니깐\n\n# imagenet 쓸때 국롤\nmean = (0.485, 0.456, 0.406)    # 왜 이렇게 하는지 물어보기. RGB값인지? \nstd = (0.229, 0.224, 0.225)\n\n\nbatch_size = 32                 \n# 한 번에 32개씩 불러온다\n",
    "\n''' ####################### 3. 이미지 불러오기 ####################### '''\n\n# 데이터로더 (데이터셋 불러오기)\ncat_directory = 'dataset/training_set/cats'\ndog_directory = 'dataset/training_set/dogs'\n\ncat_images_filepaths = sorted([os.path.join(cat_directory, f) for f in os.listdir(cat_directory)])   \ndog_images_filepaths = sorted([os.path.join(dog_directory, f) for f in os.listdir(dog_directory)])\nimages_filepaths = [*cat_images_filepaths, *dog_images_filepaths]    \ncorrect_images_filepaths = [i for i in images_filepaths if cv2.imread(i) is not None] \n# imread가 성공하면 correct_images_filepaths에 이미지를 불러온다.\n\n\n\n# 랜덤 시드 고정하고 훈련, 검증, 테스트셋 나누기\n# 개 4000개, 고양이 4000개 섞기 (총 8000개)\nrandom.seed(42)    \nrandom.shuffle(correct_images_filepaths)\ntrain_images_filepaths = correct_images_filepaths[:7000]     # 6999번째까지 훈련용\nval_images_filepaths = correct_images_filepaths[7000:-10]    # 7000번째 ~ 끝에서 10개까지 검증용(매 에포크마다 검증용) \ntest_images_filepaths = correct_images_filepaths[-10:]      # 끝에 10개는 테스트용(최종 확인용)\nprint(\"\\n### dataset load ###\")\nprint('train: ', len(train_images_filepaths), 'val: ', len(val_images_filepaths), 'test: ',len(test_images_filepaths))   # 잘 분류됐나 확인 코드\n\n\n\n# 이미지에 대한 데이터셋 클래스(레이블 구분) 만들기(이미지파일 이름으로 레이블 생성하고 나누기) \nclass DogvsCatDataset(Dataset):              # Dataset 클래스를 상속한다.  \n    def __init__(self, file_list, transform=None, phase='train'):    \n        self.file_list = file_list\n        self.transform = transform\n        self.phase = phase\n        \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, idx):       \n        img_path = self.file_list[idx]\n        img = Image.open(img_path)        \n        img_transformed = self.transform(img, self.phase)\n        \n\n        label = img_path.split('/')[-1].split('.')[0]     \n        # 이미지 데이터에 대한 레이블값을 가져온다(이미지파일 이름을 .로 구분하고 첫째껄 가져온다)\n        # 이미지의 path에서 /로 먼저 나누고 .으로 나중에 나눈다. \n        if label == 'dog':\n            label = 1\n        elif label == 'cat':\n            label = 0\n        return img_transformed, label\n\n\n# 이미지 데이터셋 정의(위에서 만든 ImageTransform, DogvsCatDataset 클래스 이용)\ntrain_dataset = DogvsCatDataset(train_images_filepaths, transform=ImageTransform(size, mean, std), phase='train')\nval_dataset = DogvsCatDataset(val_images_filepaths, transform=ImageTransform(size, mean, std), phase='val')\n\n# 첫번째 이미지 테스트(index = 0)\nindex_test = 13\nprint(\"index_test's input shape: \", train_dataset.__getitem__(index_test)[0].size())     # 출력: torch.Size([3, 224, 224]) \nprint(\"index_test's label: \", train_dataset.__getitem__(index_test)[1])                  # 출력: 0(cat) or 1(dog) \n# __getitem__: 파이썬의 특별 메소드 중 하나이다. \n# 슬라이싱을 구현할 수 있도록 도우며 리스트에서 슬라이싱을 하게되면 내부적으로 __getitem__ 메소드를 실행한다.\n# 객체에서 슬라이싱을 하기 위해선 __getitem__이 필수적이다. \n\n\n# 배치 크기만큼 나눠서 데이터를 메모리로 불러오기\ntrain_iterator  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_iterator = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ndataloader_dict = {'train': train_iterator, 'val': valid_iterator}\n\nbatch_iterator = iter(train_iterator)\ninputs, label = next(batch_iterator)\nprint(\"batch_iterator's size\", inputs.size())       # 출력: torch.Size([배치사이즈, 3, 224, 224]) \nprint(\"batch_iterator's label\", label)              # 출력: 배치 사이즈 만큼의 라벨링된 숫자\n\n\n",
    "\n# 배치로 불러온 이미지들 띄우기\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.6     # unnormalize -> 이거 왜 하는건지 물어보기\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\nimshow(torchvision.utils.make_grid(inputs))\n",
    "''' ####################### 4. 모델 정의 ####################### '''\n\n\n''' 미리 만들어진 ResNet 불러오기 '''\n# timm으로 모델 불러오기\n# import timm\n# model = timm.create_model(\"resnet50\", pretrained= False)\n# # model.fc = nn.Sequential(\n# #     nn.\n# # )\n\n\n# torchsummaryX.summary(model, torch.zeros(1,3,224,224))\n\n# model = model.to(device)\n\n# model\n\n# 모델 생성하기\nclass BasicBlock(nn.Module):    \n    expansion = 1\n    \n    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n        super().__init__()                \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n                               stride = stride, padding = 1, bias = False)\n        self.bn1 = nn.BatchNorm2d(out_channels)        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n                               stride = 1, padding = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(out_channels)        \n        self.relu = nn.ReLU(inplace = True)\n        \n        if downsample:      # 다운샘플 적용: 입력 데이터의 크기와 네트워크를 통과한 후 출력 데이터의 크기가 다를 경우에 사용\n            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n                             stride = stride, bias = False)\n            bn = nn.BatchNorm2d(out_channels)\n            downsample = nn.Sequential(conv, bn)\n        else:\n            downsample = None        \n        self.downsample = downsample\n        \n    def forward(self, x):       \n        i = x       \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        if self.downsample is not None:\n            i = self.downsample(i)\n                        \n        x += i  # 아이덴티티 매핑 적용 \n        x = self.relu(x)\n        \n        return x\n\n\n\nclass Bottleneck(nn.Module):    \n    expansion = 4   # 병목 블록을 정의하기 위한 파라미터\n    \n    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n        super().__init__()    \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = 1, bias = False)\n        self.bn1 = nn.BatchNorm2d(out_channels)        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(out_channels)        \n        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size = 1,\n                               stride = 1, bias = False)\n        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)        \n        self.relu = nn.ReLU(inplace = True)\n        \n        if downsample:\n            conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = 1, \n                             stride = stride, bias = False)\n            bn = nn.BatchNorm2d(self.expansion * out_channels)\n            downsample = nn.Sequential(conv, bn)\n        else:\n            downsample = None            \n        self.downsample = downsample\n        \n    def forward(self, x):        \n        i = x        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)        \n        x = self.conv3(x)\n        x = self.bn3(x)\n                \n        if self.downsample is not None:\n            i = self.downsample(i)\n            \n        x += i\n        x = self.relu(x)\n    \n        return x\n\n\nclass ResNet(nn.Module):\n    def __init__(self, config, output_dim, zero_init_residual=False):\n        super().__init__()\n                \n        block, n_blocks, channels = config\n        self.in_channels = channels[0]            \n        assert len(n_blocks) == len(channels) == 4\n        \n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace = True)\n        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n        \n        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride = 2)\n        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride = 2)\n        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride = 2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc = nn.Linear(self.in_channels, output_dim)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n        \n    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):   \n        layers = []        \n        if self.in_channels != block.expansion * channels:\n            downsample = True\n        else:\n            downsample = False\n        \n        layers.append(block(self.in_channels, channels, stride, downsample))\n        \n        for i in range(1, n_blocks):\n            layers.append(block(block.expansion * channels, channels))\n\n        self.in_channels = block.expansion * channels            \n        return nn.Sequential(*layers)\n        \n    def forward(self, x):        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)        \n        x = self.avgpool(x)\n        h = x.view(x.shape[0], -1)\n        x = self.fc(h)        \n        return x, h\n\nfrom collections import namedtuple      # namedtuple: 튜플의 성질을 갖고 있는 자료형이지만 인덱스뿐만 아니라 키 값으로 데이터에 접근할 수 있다.\nResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])\nresnet18_config = ResNetConfig(block = BasicBlock,\n                               n_blocks = [2,2,2,2],\n                               channels = [64, 128, 256, 512])\n\n\nOUTPUT_DIM = 2  # 2개의 클래스 사용\nmodel = ResNet(resnet18_config, OUTPUT_DIM)\nprint(model)\n",
    "\n''' 옵티마이저, 손실함수 정의 '''\n# 옵티마이저 정의\noptimizer = optim.Adam(model.parameters(), lr = 1e-4)    # 1*10의 -4제곱\n# 국롤은 -3~-4\n\n# 손실함수 정의 \ncriterion = nn.CrossEntropyLoss()   #  다중 클래스 분류를 위해 사용한다. BCELoss로도 써보기 \n\n\n''' 모델 디바이스에 전송하기 '''\nmodel = model.to(device)\ncriterion = criterion.to(device)\n\n''' 모델 요약 '''\n# from IPython.core.interactiveshell import InteractiveShell      # 주피터에서 라인을 모두 보고 싶을때 사용한다.\n# InteractiveShell.ast_node_interactivity = \"all\"\n\n# print(model)    # 모델 구성 모두 보고 싶을때\n\n# from torchsummary import summary    # 모델 요약만 보고 싶을때\n# summary(model, input_size=(3, 224, 224))\n\n\ndef count_parameters(model):           \n    return sum(p.numel() for p in model.parameters() if p.requires_grad)    # 모델의 총 파라미터 수 계산\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')",
    "''' ####################### 5. 모델 학습 ####################### '''\n\n# 모델 학습 정확도 측정 함수 정의\ndef calculate_topk_accuracy(y_pred, y, k = 2):\n    with torch.no_grad():\n        batch_size = y.shape[0]\n        _, top_pred = y_pred.topk(k, 1)\n        top_pred = top_pred.t()\n        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n        correct_1 = correct[:1].reshape(-1).float().sum(0, keepdim = True)\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim = True)\n        acc_1 = correct_1 / batch_size\n        acc_k = correct_k / batch_size\n    return acc_1, acc_k\n\n\n# 학습함수 정의\ndef train(model, iterator, optimizer, criterion, device):    \n    epoch_loss = 0\n    epoch_acc_1 = 0\n    epoch_acc_5 = 0\n    \n    model.train()    \n    for (x, y) in iterator:        \n        x = x.to(device)\n        y = y.to(device)\n            \n        optimizer.zero_grad()                \n        y_pred = model(x)  \n        \n        loss = criterion(y_pred[0], y) \n        \n        acc_1, acc_5 = calculate_topk_accuracy(y_pred[0], y)        \n        loss.backward()        \n        optimizer.step()        \n        \n        epoch_loss += loss.item()\n        epoch_acc_1 += acc_1.item()\n        epoch_acc_5 += acc_5.item()\n        \n    epoch_loss /= len(iterator)\n    epoch_acc_1 /= len(iterator)\n    epoch_acc_5 /= len(iterator)        \n    return epoch_loss, epoch_acc_1, epoch_acc_5\n\n\n# 평가함수 정의\ndef evaluate(model, iterator, criterion, device):    \n    epoch_loss = 0\n    epoch_acc_1 = 0\n    epoch_acc_5 = 0\n    \n    model.eval()    \n    with torch.no_grad():        \n        for (x, y) in iterator:\n            x = x.to(device)\n            y = y.to(device)\n            y_pred = model(x)            \n            loss = criterion(y_pred[0], y)\n            acc_1, acc_5 = calculate_topk_accuracy(y_pred[0], y)\n\n            epoch_loss += loss.item()\n            epoch_acc_1 += acc_1.item()\n            epoch_acc_5 += acc_5.item()\n        \n    epoch_loss /= len(iterator)\n    epoch_acc_1 /= len(iterator)\n    epoch_acc_5 /= len(iterator)        \n    return epoch_loss, epoch_acc_1, epoch_acc_5\n\n\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\n",
    "\n# 학습 시작\nbest_valid_loss = float('inf')\nEPOCHS = 50\n\ntrain_loss_plt = []\nvalid_loss_plt = []\ntrain_acc_plt = []\nvalid_acc_plt = []\n\nfor epoch in range(EPOCHS):    \n    start_time = time.monotonic()\n    \n    train_loss, train_acc_1, train_acc_5 = train(model, train_iterator, optimizer, criterion, device)\n    valid_loss, valid_acc_1, valid_acc_5 = evaluate(model, valid_iterator, criterion, device)\n\n    train_loss_plt.append(train_loss)\n    valid_loss_plt.append(valid_loss)\n    train_acc_plt.append(train_acc_1)\n    valid_acc_plt.append(valid_acc_1)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'ResNet-model.pth')\n\n    end_time = time.monotonic()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc @1: {train_acc_1*100:6.2f}% | ' \\\n          f'Train Acc @5: {train_acc_5*100:6.2f}%')\n    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc @1: {valid_acc_1*100:6.2f}% | ' \\\n          f'Valid Acc @5: {valid_acc_5*100:6.2f}%')\n\n\n# 손실과 정확도 그래프 출력\n\n\nplt.figure(figsize=(13, 5))\nfig, axs = plt.subplots(1, 2)\nfig.tight_layout()\n\n\nax1 = plt.subplot(1,2,1)\nplt.title('Loss Graph')\nax1.plot(train_loss_plt, c=\"blue\", label='Trainset_loss')\nax1.plot(valid_loss_plt, c=\"cornflowerblue\", label='Testset_loss')\nax1.legend(['train_loss', 'val_loss'])\nplt.xlabel('epoch')\nplt.ylabel('loss')\n\n\nax2 = plt.subplot(1,2,2)\nplt.title('Accuracy Graph')\nax2.plot(train_acc_plt, c=\"red\", label='Trainset_acc')\nax2.plot(valid_acc_plt, c=\"lightcoral\", label='Testset_acc')\nax2.legend(['train_acc', 'val_acc'])\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\n\nplt.subplots_adjust(left=0.125,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.2, \n                    hspace=0.35)\n\n\nplt.savefig('savefig_default.png')\nplt.show()\n\n\n"
]