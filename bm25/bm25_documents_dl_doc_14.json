[
    {
        "id": null,
        "metadata": {},
        "page_content": "[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14. ê³¼ì í•© ë°©ì§€ ê¸°ë²•ğŸ“˜[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/ğŸ“š[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 6ì£¼ì°¨/ğŸ“•[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14. ê³¼ì í•© ë°©ì§€ ê¸°ë²•Made withğŸ“•[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14. ê³¼ì í•© ë°©ì§€ ê¸°ë²•[ìˆ˜ì—… ëª©í‘œ]ì—¬ëŸ¬ ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ì‹œë‹¤.Pytorchë¡œ  ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì— ëŒ€í•œ ì‹¤ìŠµ ì˜ˆì‹œ![ëª©ì°¨]01.",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "ëŒ€í•œ ì‹¤ìŠµ ì˜ˆì‹œ![ëª©ì°¨]01. ê³¼ì í™” ë°©ì§€ ê¸°ë²•02. ê³¼ì í•© ë°©ì§€ê¸°ë²• ì‹¤ìŠµ(Pytorch)import torch.nn as nn",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "import torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nâ€‹â˜‘ï¸ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹PythonCopy# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\ntransform = transforms.Compose([",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n# CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "# CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "â€‹â˜‘ï¸  ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ {5px} ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ ï»¿â€‹PythonCopyclass CNNWithDropoutAndBatchNorm(nn.Module):\ndef __init__(self):",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "def __init__(self):\nsuper(CNNWithDropoutAndBatchNorm, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.fc1 = nn.Linear(128 * 56 * 56, 256)\n        self.dropout = nn.Dropout(0.5)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "self.fc2 = nn.Linear(256, 10)\ndef forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = torch.max_pool2d(x, 2)\n        x = torch.relu(self.bn2(self.conv2(x)))",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "x = torch.max_pool2d(x, 2)\n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\nreturn x",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "model = CNNWithDropoutAndBatchNorm()",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "â€‹nn.Conv2d: 2ì°¨ì› í•©ì„±ê³± ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤. nn.Conv2d(in_channels, out_channels, kernel_size, padding)ì€ ì…ë ¥ ì±„ë„ ìˆ˜, ì¶œë ¥ ì±„ë„ ìˆ˜, ì»¤ë„ í¬ê¸°, íŒ¨ë”©ì„ ì§€ì •.Conv2d(in_channels, out_channels, kernel_size, padding)ì€ ì…ë ¥ ì±„ë„ ìˆ˜, ì¶œë ¥ ì±„ë„ ìˆ˜, ì»¤ë„",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "ì±„ë„ ìˆ˜, ì¶œë ¥ ì±„ë„ ìˆ˜, ì»¤ë„ í¬ê¸°, íŒ¨ë”©ì„ ì§€ì •ï»¿â€‹nn.BatchNorm2d: 2ì°¨ì› ë°°ì¹˜ ì •ê·œí™” ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤.nn.Dropout: ë“œë¡­ì•„ì›ƒ ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤. nn.Dropout(p)ì€ ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤..Dropout(p)ì€ ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹torch.max_pool2d: 2ì°¨ì› ìµœëŒ€ í’€ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.â˜‘ï¸ ì†ì‹¤ í•¨ìˆ˜ì™€",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "í’€ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.â˜‘ï¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ {5px}ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ ï»¿â€‹PythonCopycriterion = nn.CrossEntropyLoss()",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "optimizer = optim.Adam(model.parameters(), lr=0.001)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "â€‹nn.CrossEntropyLoss: êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.optim.Adam: Adam ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì •ì˜í•©ë‹ˆë‹¤. lrì€ í•™ìŠµë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤.ì€ í•™ìŠµë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹â˜‘ï¸ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹PythonCopynum_epochs = 10",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "for i, (inputs, labels) in enumerate(trainloader):\n        inputs, labels = inputs.to(device), labels.to(device)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "running_loss += loss.item()\nif i % 100 == 99: # ë§¤ 100 ë¯¸ë‹ˆë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥\nprint(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n            running_loss = 0.0",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "print('Finished Training')",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "â€‹model.train(): ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.optimizer.zero_grad(): ì´ì „ ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.loss.backward(): ì—­ì „íŒŒë¥¼ í†µí•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.optimizer.step(): ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.â˜‘ï¸ ëª¨ë¸ í‰ê°€ëª¨ë¸ í‰ê°€ {5px}ëª¨ë¸ í‰ê°€",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "í‰ê°€ëª¨ë¸ í‰ê°€ {5px}ëª¨ë¸ í‰ê°€ ï»¿â€‹PythonCopymodel.eval()",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "correct = 0\ntotal = 0\nwith torch.no_grad():\nfor inputs, labels in testloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "_, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "â€‹model.eval(): ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.torch.no_grad(): í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ, ì´ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì…ë‹ˆë‹¤.torch.max: í…ì„œì˜ ìµœëŒ€ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤. torch.max(outputs.data, 1)ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤..max(outputs.data, 1)ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹labels.size(0): ë°°ì¹˜ í¬ê¸°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.(predicted == labels).sum().item(): ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì´ ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œì˜ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.2)  ë°ì´í„° ì¦ê°•ì„ í†µí•œ ëª¨ë¸ ì„±ëŠ¥",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "ë°ì´í„° ì¦ê°•ì„ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ì‹¤ìŠµâ˜‘ï¸ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹PythonCopy# ë°ì´í„° ì¦ê°• ì ìš©",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "transform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "transform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n# CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "# CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "â€‹transforms.RandomHorizontalFlip(): ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ìˆ˜í‰ ë°˜ì „í•©ë‹ˆë‹¤.transforms.RandomCrop(size, padding): ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ìë¥´ê³ , íŒ¨ë”©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.â˜‘ï¸  ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ì²«ë²ˆì§¸ ì‹¤ìŠµì—ì„œ ì •ì˜í•œ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ {5px} ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ ï»¿â€‹PythonCopyclass CNNWithDropoutAndBatchNorm(nn.Module):",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "def __init__(self):\nsuper(CNNWithDropoutAndBatchNorm, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.fc1 = nn.Linear(128 * 56 * 56, 256)\n        self.dropout = nn.Dropout(0.5)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "self.fc2 = nn.Linear(256, 10)\ndef forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        x = torch.max_pool2d(x, 2)\n        x = torch.relu(self.bn2(self.conv2(x)))",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "x = torch.max_pool2d(x, 2)\n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\nreturn x",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "model = CNNWithDropoutAndBatchNorm()\nâ€‹â˜‘ï¸ ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ì²«ë²ˆì§¸ ì‹¤ìŠµì—ì„œ ì •ì˜í•œ ì†ì‹¤í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ {5px}ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ ï»¿â€‹PythonCopycriterion = nn.CrossEntropyLoss()",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "optimizer = optim.Adam(model.parameters(), lr=0.001)\nâ€‹â˜‘ï¸ ëª¨ë¸ í•™ìŠµì²«ë²ˆì§¸ ì‹¤ìŠµì—ì„œ ì •ì˜í•œ ëª¨ë¸ í•™ìŠµ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.ëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹PythonCopynum_epochs = 10",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "for i, (inputs, labels) in enumerate(trainloader):\n        inputs, labels = inputs.to(device), labels.to(device)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "running_loss += loss.item()\nif i % 100 == 99: # ë§¤ 100 ë¯¸ë‹ˆë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥\nprint(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n            running_loss = 0.0",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "print('Finished Training')\nâ€‹â˜‘ï¸ ëª¨ë¸ í‰ê°€ì²«ë²ˆì§¸ ì‹¤ìŠµì—ì„œ ì •ì˜í•œ ëª¨ë¸ í‰ê°€ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.ëª¨ë¸ í‰ê°€ {5px}ëª¨ë¸ í‰ê°€ ï»¿â€‹PythonCopymodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\nfor inputs, labels in testloader:",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "correct += (predicted == labels).sum().item()\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\nâ€‹Copyright â“’ TeamSparta All rights reserved.",
        "type": "Document"
    }
]