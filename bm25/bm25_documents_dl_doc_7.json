[
    {
        "id": null,
        "metadata": {},
        "page_content": "[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘📘[SCC] 기초가 탄탄한 딥러닝/📚[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 3주차/📕[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘Made with📕[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘[수업 목표]최근 가장 성능 좋은 매커니즘! 어텐션 메커니즘에 대해",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "매커니즘! 어텐션 메커니즘에 대해 알아봅시다Pytorch의 구현 예시를 살펴봅시다[목차]01. 개념02. 실습:  Attention 메커니즘의 구현한번 훑는 정도로 넘어갑시다!1)  Attention☑️ Scaled Dot-Product AttentionScaled Dot-Product attention 메커니즘 구현{5px}Scaled",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "메커니즘 구현{5px}Scaled Dot-Product attention 메커니즘 구현﻿​PythonCopyimport torch",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "import torch.nn.functional as F",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "def scaled_dot_product_attention(Q, K, V):\n    d_k = Q.size(-1) # Key의 차원 수\n    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # 유사도 계산 및 스케일링",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "attn_weights = F.softmax(scores, dim=-1) # Softmax를 통한 가중치 계산\n    output = torch.matmul(attn_weights, V) # 가중합을 통한 최종 출력 계산\nreturn output, attn_weights",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "​☑️ Multi-Head Attention Multi-Head Attention 메커니즘 구현{5px} Multi-Head Attention 메커니즘 구현﻿​PythonCopyclass MultiHeadAttention(nn.Module):\ndef __init__(self, embed_size, heads):",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "super(MultiHeadAttention, self).__init__()\n        self.embed_size = embed_size\n        self.heads = heads\n        self.head_dim = embed_size // heads",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "assert (\n            self.head_dim * heads == embed_size\n        ), \"Embedding size needs to be divisible by heads\"",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\ndef forward(self, values, keys, query, mask=None):",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "N = query.shape[0]\n        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n# Linear transformations",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "values = self.values(values).view(N, value_len, self.heads, self.head_dim)\n        keys = self.keys(keys).view(N, key_len, self.heads, self.head_dim)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "queries = self.queries(query).view(N, query_len, self.heads, self.head_dim)\n# Scaled dot-product attention\n        out, _ = scaled_dot_product_attention(queries, keys, values)",
        "type": "Document"
    },
    {
        "id": null,
        "metadata": {},
        "page_content": "out = out.view(N, query_len, self.heads * self.head_dim)\n        out = self.fc_out(out)\nreturn out\n\n​Copyright ⓒ TeamSparta All rights reserved.",
        "type": "Document"
    }
]